{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules and functions\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from pysam import FastaFile\n",
    "import random\n",
    "import scipy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import umap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from collections import Counter\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from numba import cuda \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib as mpl\n",
    "\n",
    "# set tick width\n",
    "mpl.rcParams['xtick.major.size'] = 20\n",
    "mpl.rcParams['xtick.major.width'] = 4\n",
    "mpl.rcParams['xtick.minor.size'] = 10\n",
    "mpl.rcParams['xtick.minor.width'] = 2\n",
    "\n",
    "mpl.rcParams['ytick.major.size'] = 20\n",
    "mpl.rcParams['ytick.major.width'] = 4\n",
    "mpl.rcParams['ytick.minor.size'] = 10\n",
    "mpl.rcParams['ytick.minor.width'] = 2\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest,chi2 \n",
    "from sklearn import svm \n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,MaxAbsScaler,RobustScaler\n",
    "from sklearn.model_selection import train_test_split,KFold,GridSearchCV,cross_val_score,StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer,confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "filenames=[]\n",
    "for i in os.listdir(\"Data/\"):\n",
    "    if len(i.strip('.csv'))==7:\n",
    "        filenames.append(\"Data/\"+i)\n",
    "filenames\n",
    "x=pd.read_csv(\"Data/SRR8788980.csv\")\n",
    "x\n",
    "x.columns\n",
    "y=np.array(x[\"DANG_PANCREAS\"])\n",
    "x=x.drop([\"Unnamed: 0\"],axis=1)\n",
    "y=np.array(y).reshape(16109   ,)\n",
    "y_1=np.where(y<=0.5)[0]\n",
    "y_2=np.where(y>=0.5)[0]\n",
    "len(y_1),len(y_2)\n",
    "index=np.random.choice(y_1,5025, replace=False)\n",
    "index=np.concatenate((index, y_2), axis=0)\n",
    "index\n",
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "    originalclass.extend(y_true)\n",
    "    predictedclass.extend(y_pred)\n",
    "    return accuracy_score(y_true, y_pred) # return accuracy score\n",
    "def confidence_score(y_true,y_pred_prob):\n",
    "\n",
    "    keys=[0,1]\n",
    "    dict={}\n",
    "    i=0\n",
    "    for key in keys:\n",
    "        dict[key]=i\n",
    "        i+=1\n",
    "\n",
    "    cs=0.0\n",
    "    for i in range(len(y_true)):\n",
    "        cs+=y_pred_prob[i][dict[y_true[i]]]\n",
    "\n",
    "\n",
    "    return cs/len(y_true)\n",
    "x_32=x[[ 'AA_x', 'AT_x', 'AG_x', 'AC_x',\n",
    "       'TA_x', 'TT_x', 'TG_x', 'TC_x', 'GA_x', 'GT_x', 'GG_x', 'GC_x', 'CA_x',\n",
    "       'CT_x', 'CG_x', 'CC_x', 'AA_y', 'AT_y', 'AG_y', 'AC_y', 'TA_y', 'TT_y',\n",
    "       'TG_y', 'TC_y', 'GA_y', 'GT_y', 'GG_y', 'GC_y', 'CA_y', 'CT_y', 'CG_y',\n",
    "       'CC_y']]\n",
    "x_32\n",
    "y_32=y\n",
    "def normalize(x):\n",
    "    new_x=[]\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        x_new=[]\n",
    "        for j in range(16,32):\n",
    "            a=np.sum(x[i][0:16])\n",
    "            b=np.sum(x[i][16:32])\n",
    "            if x[i][j-16]!=0:\n",
    "                x_new.append(x[i][j]*a/( x[i][j-16]*b))\n",
    "            else:\n",
    "                x_new.append(0.0)\n",
    "        new_x.append(x_new)\n",
    "    return new_x\n",
    "x_32.shape\n",
    "x_32_norm=np.array(normalize(np.array(x_32)))\n",
    "x_32_norm.shape\n",
    "title=['AA', 'AT', 'AG', 'AC',\n",
    "       'TA', 'TT', 'TG', 'TC', 'GA', 'GT', 'GG', 'GC', 'CA',\n",
    "       'CT', 'CG', 'CC']\n",
    "np.sort(title)\n",
    "x.iloc[np.where(x_32_norm[:,3]>100)][[\"AA_x\",\"AA_y\"]]\n",
    "plt.subplots(4,4,figsize=(20,20))\n",
    "for i in range(16):\n",
    "    plt.title(title[i])\n",
    "    plt.subplot(4,4,i+1)\n",
    "    CG=x_32_norm[:,i]\n",
    "    print(title[i],\": \",np.mean(CG[y==0]),np.mean(CG[y==1]),100*(np.mean(CG[y==1])-np.mean(CG[y==0]))/np.mean(CG[y==0]),\"%\")\n",
    "    plt.violinplot((CG[y==0],CG[y==1]))\n",
    "plt.show()\n",
    "x_32_norm,y\n",
    "# drop y with neagtive values remove also the corresponding x\n",
    "index=np.where(y>=0)[0]\n",
    "x_32_norm=x_32_norm[index]\n",
    "y=y[index]\n",
    "# ML models\n",
    "# Naive Bayes\n",
    "clf=MultinomialNB(fit_prior=True, class_prior=None)  \n",
    "clf_parameters = {\n",
    "        'clf__alpha':(0.1,0.5,1,10,0.01,100,0.001),\n",
    "        }\n",
    "pipeline = Pipeline([\n",
    "    ('clf', clf)]) \n",
    "\n",
    "\n",
    "parameters={**clf_parameters} \n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "grid = GridSearchCV(pipeline,parameters,scoring='f1_micro',cv=cv)          \n",
    "grid.fit(x_32_norm,y)     \n",
    "clf_nb=grid.best_estimator_\n",
    "clf_nb\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "nested_score = cross_val_score(clf_nb, X=x_32_norm, y=y, cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "\n",
    "print(classification_report(originalclass, predictedclass))\n",
    "x_norm =StandardScaler().fit_transform(x_32_norm)\n",
    "# Logistic regression\n",
    "clf=LogisticRegression(class_weight='balanced',max_iter=1000) \n",
    "clf_parameters = {\n",
    "        'clf__solver':('newton-cg','lbfgs','liblinear'),\n",
    "        'clf__C':[1,0.1,0.01]\n",
    "        }\n",
    "\n",
    "pipeline = Pipeline([   \n",
    "    ('clf', clf),]) \n",
    "parameters={**clf_parameters} \n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "grid = GridSearchCV(pipeline,parameters,scoring='f1_macro',cv=cv)          \n",
    "grid.fit(x_norm,y)     \n",
    "clf_lr= grid.best_estimator_  \n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "nested_score = cross_val_score(clf_lr, X=x, y=y, cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "\n",
    "print(classification_report(originalclass, predictedclass))\n",
    "clf_lr.steps[0][1]\n",
    "importance = clf_lr.steps[0][1].coef_\n",
    "plt.bar(title , [ np.linalg.norm(importance[:,i])  for i in range(len(importance[0]))] )\n",
    "plt.xlabel(\"Model Parameters\")\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.show()\n",
    "y_predict_prob=clf_lr.steps[0][1].predict_proba(x_norm)\n",
    "confidence_score(y,y_predict_prob)\n",
    "# Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=40)\n",
    "clf_parameters = {\n",
    "        'clf__criterion':('gini', 'entropy'), \n",
    "        'clf__max_features':( 'sqrt', 'log2'),\n",
    "        'clf__ccp_alpha':(0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1),\n",
    "        'clf__max_depth':(10,40,45,60),\n",
    "        }\n",
    "pipeline = Pipeline([\n",
    "    ('clf', clf)]) \n",
    "parameters={**clf_parameters} \n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "grid = GridSearchCV(pipeline,parameters,scoring='f1_macro',cv=cv)          \n",
    "grid.fit(x_norm,y)     \n",
    "clf_dtc = grid.best_estimator_ \n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "nested_score = cross_val_score(clf_dtc , X=x_norm, y=y, cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "\n",
    "print(classification_report(originalclass, predictedclass))\n",
    "importance = clf_dtc.steps[0][1].feature_importances_\n",
    "plt.bar(title, importance)\n",
    "plt.xlabel(\"Model Parameters\")\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.show()\n",
    "y_predict_prob=clf_dtc.steps[0][1].predict_proba(x_norm)\n",
    "confidence_score(y,y_predict_prob)\n",
    "# SVC\n",
    "clf = svm.SVC(class_weight='balanced',max_iter=5000,probability=True)  \n",
    "clf_parameters = {\n",
    "        'clf__C':(0.1,1,2,10,50,100),\n",
    "        \"clf__kernel\":('linear','rbf','poly','sigmoid')\n",
    "        }\n",
    "pipeline = Pipeline([\n",
    "    ('clf', clf)]) \n",
    "\n",
    "parameters={**clf_parameters} \n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "print(cv) \n",
    "grid = GridSearchCV(pipeline,parameters,scoring='f1_micro',cv=cv)          \n",
    "grid.fit(x_norm,y)     \n",
    "clf_svc= grid.best_estimator_ \n",
    "clf_svc.steps[0][1].decision_function_shape\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "nested_score = cross_val_score(clf_svc, X=x, y=y, cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "\n",
    "print(classification_report(originalclass, predictedclass))\n",
    "# Deep Learning\n",
    "clf_dl = MLPClassifier(hidden_layer_sizes=(100,50,10,1),activation='logistic',solver=\"adam\",batch_size=10,learning_rate=\"adaptive\",max_iter=1000)\n",
    "clf_parameters = {\n",
    "                    'clf__activation':(['logistic']) #'tanh', 'relu',\n",
    "                    }\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('clf', clf)]) \n",
    "\n",
    "parameters={**clf_parameters} \n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "grid = GridSearchCV(pipeline,parameters,scoring='f1_micro',cv=cv)          \n",
    "grid.fit(x,y)     \n",
    "clf= grid.best_estimator_ \n",
    "clf_dl\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "nested_score = cross_val_score(clf_dl, X=x, y=y, cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "\n",
    "print(classification_report(originalclass, predictedclass))\n",
    "print(classification_report(originalclass, predictedclass))\n",
    "# clf_dl.fit(x,y)\n",
    "plt.plot(clf.steps[0][1].loss_curve_)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "print(classification_report(originalclass, predictedclass))\n",
    "def classifier(x):\n",
    "    classify = tf.keras.Sequential([ layers.Dense(1,activation='softmax',use_bias=False  )])\n",
    "    output = classify(x)\n",
    "        \n",
    "    model = Model(inputs=x, outputs=output)\n",
    "\n",
    "    return model\n",
    "model = classifier(layers.Input(shape=(16,)) )\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07 \n",
    "    ) , loss=  'categorical_crossentropy' ,metrics = 'accuracy')\n",
    "history = model.fit( x_norm,y   , batch_size=50,\n",
    "                epochs=10, \n",
    "                )\n",
    "model.weights\n",
    "# Random forest classifier\n",
    "clf = RandomForestClassifier(class_weight='balanced')\n",
    "clf_parameters = {\n",
    "                    'clf__criterion':('gini', 'entropy'), \n",
    "                    'clf__max_features':( 'sqrt', 'log2'),   \n",
    "                    'clf__n_estimators':(30,50,100,200,400,800,1000,1200),\n",
    "                    'clf__max_depth':(2,5,10,15,20,25,30),\n",
    "                    }\n",
    "pipeline = Pipeline([\n",
    "    ('clf', clf)]) \n",
    "\n",
    "parameters={**clf_parameters} \n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "grid = GridSearchCV(pipeline,parameters,scoring='f1_micro',cv=cv)          \n",
    "grid.fit(x_norm,y)     \n",
    "clf_rf= grid.best_estimator_ \n",
    "clf_rf\n",
    "grid.best_estimator_ \n",
    "clf_rf.steps[0][1]\n",
    "clf\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "nested_score = cross_val_score(clf_rf, X=x, y=y, cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "outer_cv\n",
    "print(classification_report(originalclass, predictedclass))\n",
    "importance = clf_rf.steps[0][1].feature_importances_\n",
    "plt.bar(title, importance)\n",
    "plt.xlabel(\"Model Parameters\")\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.show()\n",
    "# Use on other data\n",
    "model = LogisticRegression() #RandomForestClassifier(class_weight='balanced', max_depth=20, n_estimators=400)\n",
    "model.fit(x_norm,y)\n",
    "y\n",
    "model.predict_proba(x_norm)[:,1]\n",
    "model.predict(x_norm)\n",
    "def data_extraction_for_ML(x):\n",
    "    x_32=x[[ 'AA_x', 'AT_x', 'AG_x', 'AC_x',\n",
    "       'TA_x', 'TT_x', 'TG_x', 'TC_x', 'GA_x', 'GT_x', 'GG_x', 'GC_x', 'CA_x',\n",
    "       'CT_x', 'CG_x', 'CC_x', 'AA_y', 'AT_y', 'AG_y', 'AC_y', 'TA_y', 'TT_y',\n",
    "       'TG_y', 'TC_y', 'GA_y', 'GT_y', 'GG_y', 'GC_y', 'CA_y', 'CT_y', 'CG_y',\n",
    "       'CC_y']]\n",
    "    x_32_norm=np.array(normalize(np.array(x_32)))\n",
    "    \n",
    "    x_norm =StandardScaler().fit_transform(x_32_norm)\n",
    "   \n",
    "    return x_norm\n",
    "xc.shape\n",
    "#accuracy_score(y,model.predict(xc))\n",
    "xtfull=data_extraction_for_ML( pd.read_csv('/project/ReadStatistics-data/TAMIL_FINALFILE.csv',low_memory=False))\n",
    "xcfull=data_extraction_for_ML( pd.read_csv('/project/ReadStatistics-data/CEPH_FINALFILE.csv',low_memory=False))\n",
    "xjfull=data_extraction_for_ML( pd.read_csv('/project/ReadStatistics-data/JAPAN_FINALFILE.csv',low_memory=False))\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.scatter(np.arange(50),model.predict(xtfull)[0:50],label='TAMIL')\n",
    "plt.scatter(np.arange(50),model.predict(xcfull)[0:50],label='CEPH')\n",
    "plt.scatter(np.arange(50),model.predict(xjfull)[0:50],label='JAPAN')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "data = pd.read_csv('/project/ReadStatistics-data/CEPH_FINALFILE.csv',low_memory=False)\n",
    "merged_data = pd.merge(data, pd.read_csv(\"/project/ReadStatistics-data/hg38_methylation_level.csv\")[['chr','start','end','methy']] , on=['chr','start','end'])\n",
    "merged_data\n",
    "xt=data_extraction_for_ML(merged_data)\n",
    "merged_data\n",
    "binary_output = np.array(merged_data['methy'])\n",
    "binary_output[binary_output<0.5]=0\n",
    "binary_output[binary_output>0.5]=1\n",
    "model.predict(xt).astype(int)\n",
    "np.array(binary_output)\n",
    "accuracy_score(model.predict(xt).astype(np.int32) , np.array(binary_output).astype(np.int32) )\n",
    "accuracy_score(model.predict(xt).astype(np.int32) , np.array(binary_output).astype(np.int32) )\n",
    "plt.hist(binary_output)\n",
    "plt.hist(model.predict(xt))\n",
    "plt.show()\n",
    "plt.hist(merged_data['categorize'])\n",
    "plt.hist(model.predict(xt))\n",
    "\n",
    "plt.show()\n",
    "plt.plot(model.predict(xjfull)[0:500])\n",
    "plt.plot(model.predict(xtfull)[0:500])\n",
    "plt.plot(model.predict(xcfull)[0:500])\n",
    "\n",
    "# Gradient boosting classifier\n",
    "clf = GradientBoostingClassifier()\n",
    "clf_parameters = {\n",
    "                    'clf__learning_rate':(0.01,0.1,1,10), \n",
    "                    'clf__max_features':( 'sqrt', 'log2'),   \n",
    "                    'clf__n_estimators':(30,50,100,200,500,800,1000,1200,1400),\n",
    "                    'clf__max_depth':(2,5,10,15,20,25,30),\n",
    "                    }\n",
    "pipeline = Pipeline([\n",
    "    ('clf', clf)]) \n",
    "\n",
    "parameters={**clf_parameters} \n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "grid = GridSearchCV(pipeline,parameters,scoring='f1_micro',cv=cv)          \n",
    "grid.fit(x_norm,y)     \n",
    "clf_gb= grid.best_estimator_ \n",
    "clf_gb\n",
    "clf.steps[0][1]\n",
    "len(x) , len(y)\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "nested_score = cross_val_score(clf_gb, X=x_norm, y=y, cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "\n",
    "print(classification_report(originalclass, predictedclass))\n",
    "print(classification_report(originalclass, predictedclass))\n",
    "importance = clf_gb.steps[0][1].feature_importances_\n",
    "plt.bar(title, importance)\n",
    "plt.xlabel(\"Model Parameters\")\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.show()\n",
    "# Models\n",
    "models = [clf_nb,clf_lr,clf_dtc,clf_svc,clf_dl,clf_rf,clf_gb]\n",
    "clf_gb\n",
    "# roc curves\n",
    "x_32_norm,y\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import time\n",
    "\n",
    "originalclass_models = []\n",
    "predictedprob_models = []\n",
    "\n",
    "time_benchmark=[]\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "originalclass=[]\n",
    "predictedprob=[]\n",
    "\n",
    "print(models[0])\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "for train_index,test_index in outer_cv.split(x,y):\n",
    "    x_train=x[train_index]\n",
    "    y_train=y[train_index]\n",
    "\n",
    "    x_test=x[test_index]\n",
    "    y_test=y[test_index]\n",
    "\n",
    "    models[0].fit(x_train,y_train)\n",
    "\n",
    "    originalclass.extend(y_test)\n",
    "    predictedprob.extend(models[0].predict_proba(x_test))\n",
    "    \n",
    "end=time.time()\n",
    "\n",
    "time_benchmark.append(end-start)\n",
    "\n",
    "originalclass_models.append(originalclass)\n",
    "predictedprob_models.append(predictedprob)\n",
    "labels_space=[i.replace(\" \", \"\\n\") for i in labels]\n",
    "time_benchmark\n",
    "# time for training\n",
    "plt.figure(figsize=(13,7))\n",
    "plt.scatter(labels_space,np.array(time_benchmark)/10,s=100)\n",
    "plt.xlabel(\"Models\",fontsize=30)\n",
    "plt.ylabel(\"Time (seconds)\",fontsize=30)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"TIME.pdf\",dpi=500)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "def binary(x):\n",
    "    x=np.array(x)\n",
    "    x[x>0.5]=1.0\n",
    "    x[x<0.5]=0.0\n",
    "    return x\n",
    "labels\n",
    "for i in range(7):\n",
    "    print(classification_report(originalclass_models[i],np.argmax(predictedprob_models[i],axis=1) ))\n",
    "models = [ Pipeline(steps=[('clf', MultinomialNB(alpha=10))]),\n",
    "Pipeline(steps=[('clf',\n",
    "                 LogisticRegression(C=0.01, class_weight='balanced',\n",
    "                                    max_iter=1000, solver='liblinear'))]),\n",
    "Pipeline(steps=[('clf',\n",
    "                 DecisionTreeClassifier(ccp_alpha=0.01, max_depth=40,\n",
    "                                        max_features='sqrt',\n",
    "                                        random_state=40))]),\n",
    "Pipeline(steps=[('clf',\n",
    "                 SVC(C=2, class_weight='balanced', max_iter=5000,\n",
    "                     probability=True))]),\n",
    "MLPClassifier(activation='logistic', batch_size=10,\n",
    "              hidden_layer_sizes=(100, 50, 10, 1), learning_rate='adaptive',\n",
    "              max_iter=1000),\n",
    "Pipeline(steps=[('clf',\n",
    "                 RandomForestClassifier(class_weight='balanced', max_depth=20,\n",
    "                                        n_estimators=400))]),\n",
    "Pipeline(steps=[('clf',\n",
    "                 GradientBoostingClassifier(learning_rate=0.01, max_depth=20,\n",
    "                                            max_features='log2',\n",
    "                                            n_estimators=1000))]) ]\n",
    "for i in models:\n",
    "    print(i)\n",
    "x = x_32_norm\n",
    "for i in range(1,7):\n",
    "\n",
    "    outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    originalclass=[]\n",
    "    predictedprob=[]\n",
    "    \n",
    "    print(models[i])\n",
    "    \n",
    "    model=models[i]\n",
    "    \n",
    "    start=time.time()\n",
    "\n",
    "\n",
    "    for train_index,test_index in outer_cv.split(x_norm,y):\n",
    "        x_train=x[train_index]\n",
    "        y_train=y[train_index]\n",
    "\n",
    "        x_test=x[test_index]\n",
    "        y_test=y[test_index]\n",
    "\n",
    "        model.fit(x_train,y_train)\n",
    "\n",
    "        originalclass.extend(y_test)\n",
    "        predictedprob.extend(model.predict_proba(x_test))\n",
    "        \n",
    "    end=time.time()\n",
    "\n",
    "    time_benchmark.append(end-start)\n",
    "    \n",
    "    originalclass_models.append(originalclass)\n",
    "    predictedprob_models.append(predictedprob)\n",
    "    \n",
    "    print(\"END\")\n",
    "# binarize y\n",
    "y[y>0.5]=1\n",
    "y[y<0.5]=0\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create checkpoint directory if not exists\n",
    "checkpoint_dir = \"checkpoint_singlesample\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for i in range(1, 7):\n",
    "    outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    originalclass = []\n",
    "    predictedprob = []\n",
    "    \n",
    "    print(f\"Training model: {models[i]}\")\n",
    "\n",
    "    model = models[i]\n",
    "    start = time.time()\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(outer_cv.split(x, y)):\n",
    "        x_train, y_train = x[train_index], y[train_index]\n",
    "        x_test, y_test = x[test_index], y[test_index]\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        originalclass.extend(y_test)\n",
    "        predictedprob.extend(model.predict_proba(x_test))\n",
    "\n",
    "        # Save model at each fold\n",
    "        joblib.dump(model, f\"{checkpoint_dir}/model_{i}_fold_{fold}.joblib\")\n",
    "\n",
    "    end = time.time()\n",
    "    time_benchmark.append(end - start)\n",
    "\n",
    "    originalclass_models.append(originalclass)\n",
    "    predictedprob_models.append(predictedprob)\n",
    "\n",
    "    # Save checkpoint results after training each model\n",
    "    np.savez(f\"{checkpoint_dir}/checkpoint_model_{i}.npz\", \n",
    "             originalclass=np.array(originalclass, dtype=object), \n",
    "             predictedprob=np.array(predictedprob, dtype=object), \n",
    "             time_taken=end - start)\n",
    "\n",
    "    print(\"Model\", i, \"training complete and checkpoint saved.\")\n",
    "import glob\n",
    "import joblib\n",
    "\n",
    "\n",
    "models_restored = []\n",
    "for i in range(1, 7):  # Model indices\n",
    "    checkpoint_files = sorted(glob.glob(f\"Checkpoints/model_{i}_fold_*.joblib\"))\n",
    "    if checkpoint_files:\n",
    "        latest_checkpoint = checkpoint_files[-1]  # Get the latest fold\n",
    "        models_restored.append(joblib.load(latest_checkpoint))\n",
    "        print(f\"Loaded Model {i} from {latest_checkpoint}\")\n",
    "    else:\n",
    "        models_restored.append(None)  # Placeholder if no model found\n",
    "len(x_32_norm), len(y)\n",
    "x_32_norm[0]\n",
    "models\n",
    "originalclass_models = []\n",
    "predictedprob_models = []\n",
    "time_benchmark = []\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "for idx, model in enumerate(models_restored):\n",
    "    print(f\"Evaluating model {idx+1}\")\n",
    "\n",
    "    originalclass = []\n",
    "    predictedprob = []\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    j = 0\n",
    "    for train_index, test_index in outer_cv.split(x_32_norm, y):\n",
    "        model_dir = f\"Checkpoints/model_{idx+1}_fold_{j}.joblib\"\n",
    "        model = joblib.load(model_dir)\n",
    "        j += 1\n",
    "        # load model for the fold and checkpoint\n",
    "        print(train_index)\n",
    "        x_test = x_32_norm[test_index]\n",
    "        y_test = y[test_index]\n",
    "\n",
    "        originalclass.extend(y_test)\n",
    "        predictedprob.extend(model.predict_proba(x_test))  \n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    time_benchmark.append(end - start)\n",
    "\n",
    "    originalclass_models.append(originalclass)\n",
    "    predictedprob_models.append(predictedprob)\n",
    "\n",
    "print(\"Evaluation Complete âœ…\")\n",
    "len(models)\n",
    "labels=[\"Logistic Regression\",\"Decision Tree Classifier\",\"SVC\",\"MLP Classifier\",\"Random Forest Classifier\",\"Gradient Boosting Classifier\"]\n",
    "len(originalclass_models)\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "\n",
    "for i in range(6):\n",
    "    \n",
    "    fpr, tpr, roc_auc = roc_curve(originalclass_models[i],np.array(predictedprob_models[i])[:,1])\n",
    "    plt.plot(fpr, tpr,label= labels[i] +\" AUC: \"+str(round(roc_auc_score(originalclass_models[i],np.array(predictedprob_models[i])[:,1]),2)),linewidth=4.0)\n",
    "    \n",
    "\n",
    "plt.plot([0,1],[0,1],label=\"Random Classifier AUC: 0.50\",linestyle = '--',linewidth=4.0,color=\"black\")    \n",
    "    \n",
    "plt.legend( prop={'size': 20})\n",
    "plt.xlabel(\"False positive rate\",fontsize=30)\n",
    "plt.ylabel(\"True positive rate\",fontsize=30)\n",
    "\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "plt.savefig(\"Figures/ROC.pdf\",dpi=500)\n",
    "\n",
    "plt.show()\n",
    "np.savez(\"Figures/roc_data.npz\", \n",
    "         originalclass_models=np.array(originalclass_models),  \n",
    "         predictedprob_models=np.array(predictedprob_models),  \n",
    "         labels=np.array(labels))\n",
    "originalclass_models\n",
    "from sklearn.metrics import precision_recall_curve , average_precision_score\n",
    "# pr curves\n",
    "plt.figure(figsize=(15,10))\n",
    "for i in range(6):\n",
    "    \n",
    "    precision, recall, thresholds  = precision_recall_curve(originalclass_models[i],np.array(predictedprob_models[i])[:,1])\n",
    "    plt.plot(recall,precision,label= labels[i] +\" AP: \"+str(round(average_precision_score(originalclass_models[i],np.array(predictedprob_models[i])[:,1]),2)),linewidth=4.0)\n",
    "    \n",
    "plt.legend( prop={'size': 20})\n",
    "plt.xlabel(\"Recall\",fontsize=30)\n",
    "plt.ylabel(\"Precision\",fontsize=30)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "plt.savefig(\"Figures/PR.pdf\")\n",
    "\n",
    "plt.show()\n",
    "# Feature importances \n",
    "from sklearn.inspection import permutation_importance\n",
    "features=[]\n",
    "for i in \"ATGC\":\n",
    "    for j in \"ATGC\":\n",
    "        features.append(i+j)\n",
    "\n",
    "features_sort=[]\n",
    "for i in \"ACGT\":\n",
    "    for j in \"ACGT\":\n",
    "        features_sort.append(i+j)\n",
    "    \n",
    "index_sorted=[]\n",
    "for i in range(16):\n",
    "    for j in range(16):\n",
    "        if features[i]==features_sort[j]:\n",
    "            index_sorted.append(j) \n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "j=0\n",
    "for i in results:\n",
    "    plt.bar( np.arange(0,32,2)+j*0.1,np.mean(i[3],axis=0)[index_sorted],label=cancer_types[j], width = 0.05)\n",
    "    j+=1\n",
    "    \n",
    "plt.bar( np.arange(0,32,2)+j*0.1,np.mean(results_human_20[0][3],axis=0)[index_sorted],label=\"HUMAN BLOOD 20 SAMPLES\", width = 0.05)\n",
    "\n",
    "plt.xticks(np.arange(0,32,2)+5*0.1,np.array(features)[index_sorted],fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"\\nFeatures\",fontsize=20)\n",
    "plt.ylabel(\"\\nFeature importances\",fontsize=20)\n",
    "plt.legend(bbox_to_anchor=(1.0, 1.0),title=\"Sample names\",fontsize=15,title_fontsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.savefig(\"/project/ReadStatistics/ML_Cancer_multiple/feature_importances.png\",dpi=800)\n",
    "\n",
    "plt.show()\n",
    "# plot the feature importance for each model\n",
    "\n",
    "for i in range(5):\n",
    "    importance = feature_importances[i]\n",
    "    plt.bar(title, importance, label = labels[i])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Model Parameters\")\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.title(\"Feature Importance of Different Models\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
